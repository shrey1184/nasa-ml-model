# NASA Climate Trends Prediction - ML Project

Complete pipeline for collecting and processing NASA POWER climate data for ML model training.

## ğŸš€ Quick Start

```bash
# Activate virtual environment
source venv/bin/activate

# Make predictions with trained model
python local_inference.py

# Fetch new climate data
python main_pipeline.py --grid cities --start 2010 --end 2024

# Clean and process data
python cleaning_pipeline.py

# Run tests
python test_project.py
```

## ğŸ¤– Model Performance

**LSTM Model** for climate anomaly prediction:
- **Temperature Anomaly**: RÂ² = 0.35, RMSE = 0.107
- **Precipitation Anomaly**: RÂ² = 0.79, RMSE = 0.237
- **Architecture**: LSTM with 18 input features, 2 outputs
- **Training**: 1,260 samples, 100 epochs

## ğŸ“ Project Structure

```
ml nasa/
â”œâ”€â”€ main_pipeline.py          # Data collection pipeline
â”œâ”€â”€ cleaning_pipeline.py      # Data cleaning workflow
â”œâ”€â”€ local_inference.py        # Run model predictions
â”œâ”€â”€ test_project.py          # Comprehensive test suite
â”œâ”€â”€ nasa_apis.py             # NASA API configuration
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ setup.sh                 # Setup automation script
â”‚
â”œâ”€â”€ data/                    # Data storage
â”‚   â”œâ”€â”€ climate_model_ready_transformed.csv  # ML-ready data (1,800 samples)
â”‚   â”œâ”€â”€ model_configuration.json             # Model config
â”‚   â”œâ”€â”€ locations_major_cities.csv           # Location data
â”‚   â””â”€â”€ raw/                                 # Raw API responses
â”‚
â”œâ”€â”€ models/                  # Trained models
â”‚   â”œâ”€â”€ climate_lstm_model.keras             # LSTM model (3.79 MB)
â”‚   â”œâ”€â”€ lstm_scaler.pkl                      # Feature scaler
â”‚   â”œâ”€â”€ lstm_model_metadata.json             # Performance metrics
â”‚   â””â”€â”€ lstm_training_history.pkl            # Training logs
â”‚
â”œâ”€â”€ src/                     # Source modules
â”‚   â”œâ”€â”€ location_grid.py     # Location grid generator
â”‚   â”œâ”€â”€ data_fetcher.py      # NASA API data fetcher
â”‚   â”œâ”€â”€ data_processor.py    # Data processing
â”‚   â””â”€â”€ data_cleaner.py      # Data cleaning
â”‚
â”œâ”€â”€ results/                 # Model outputs
â”‚   â”œâ”€â”€ climate_predictions_lstm_local.csv
â”‚   â””â”€â”€ local_predictions_visualization.png
â”‚
â””â”€â”€ venv/                    # Virtual environment
```

## ğŸŒ Location Grids

The pipeline supports three types of location grids:

### 1. Major Cities (--grid cities)
- 10 major cities worldwide
- Best for: Quick testing, city-specific predictions
- Locations: New Delhi, Mumbai, Bangalore, Chennai, Kolkata, NYC, LA, London, Tokyo, Sydney

### 2. India Regional Grid (--grid india)
- 2Â° x 2Â° grid covering India
- Bounds: 8Â°N-37Â°N, 68Â°E-97Â°E
- ~300 locations
- Best for: Regional India climate modeling

### 3. Global Grid (--grid global)
- 10Â° x 10Â° global coverage
- ~650 locations
- Best for: Global climate trend analysis

## ğŸ“Š Pipeline Steps

### Step 1: Generate Location Grid
Automatically generates latitude/longitude coordinates based on selected grid type.

### Step 2: Fetch Climate Data
For each location, fetches NASA POWER data:
- **Temporal Monthly API** (2005-2024)
  - 7 parameters: T2M, T2M_MAX, T2M_MIN, PRECTOTCORR, ALLSKY_SFC_SW_DWN, RH2M, QV2M
  - Monthly time-series data

### Step 3: Process & Create Master Dataset
- Merges location metadata
- Calculates derived features:
  - T2M_range (temperature range)
  - heat_index (simplified)
  - precip_log (log-transformed precipitation)
  - Season and month indicators
- Optional: Temperature anomalies (requires baseline period data)
- Optional: Lag features for time-series modeling

## ğŸ”§ Usage Examples

### Make Predictions
```bash
python local_inference.py
# Generates predictions for all 1,800 samples
# Outputs: results/climate_predictions_lstm_local.csv
#          results/local_predictions_visualization.png
```

### Test Everything
```bash
python test_project.py
# Runs 8 comprehensive tests
# Verifies all components work correctly
```

### Fetch New Data - Test Run (3 locations, 2 years)
```bash
python main_pipeline.py --grid cities --start 2020 --end 2021 --test
```

### Fetch New Data - Major Cities (Full Period)
```bash
python main_pipeline.py --grid cities --start 2010 --end 2024
```

### India Regional Grid (Limited Locations)
```bash
python main_pipeline.py --grid india --start 2010 --end 2024 --max-locations 50
```

## ğŸ“ˆ Output Datasets

### 1. Location Grid CSV
Format: `locations_[grid_type].csv`
- location_id, latitude, longitude, grid_type, description

### 2. Raw Climate Data CSV
Format: `climate_data_[grid_type]_[start]_[end].csv`
- All raw NASA POWER parameters
- time, lat, lon, location_id, climate parameters

### 3. Master Dataset CSV
Format: `climate_master_[grid_type]_[start]_[end].csv`
- Processed data with derived features
- Ready for ML model training
- Columns: 19 features including:
  - Original climate parameters (7)
  - Location metadata (lat, lon, location_id, description)
  - Time features (time, month, season, year)
  - Derived features (T2M_range, heat_index, precip_log)

## ğŸŒ¡ï¸ Climate Parameters

### Core Parameters
1. **T2M** - Mean temperature at 2m (Â°C)
2. **T2M_MAX** - Maximum temperature at 2m (Â°C)
3. **T2M_MIN** - Minimum temperature at 2m (Â°C)

### Hydrology
4. **PRECTOTCORR** - Corrected precipitation (mm/month)

### Radiation
5. **ALLSKY_SFC_SW_DWN** - Surface shortwave radiation (W/mÂ²)

### Moisture
6. **RH2M** - Relative humidity at 2m (%)
7. **QV2M** - Specific humidity at 2m (g/kg)

## âš™ï¸ Command-Line Options

```
Options:
  --grid {global,india,cities}
                        Type of location grid (default: cities)
  --start YEAR          Start year for data collection (default: 2010)
  --end YEAR            End year for data collection (default: 2024)
  --test                Test mode: only fetch 3 locations
  --max-locations N     Maximum number of locations to fetch
```

## ğŸ“ Notes

- API requests are rate-limited (1 second delay between requests)
- NASA POWER API includes month "13" as annual average (automatically filtered)
- Raw JSON responses saved in `data/raw/` for debugging
- Test mode recommended before full runs
- Global grid (~650 locations) takes 10-12 hours to complete

## ğŸ”„ Next Steps

After generating the master dataset:
1. Perform exploratory data analysis (EDA)
2. Engineer additional features
3. Train ML models (regression, time-series, etc.)
4. Validate predictions
5. Deploy models

## ğŸ“¦ Dependencies

All dependencies are installed in `venv/`:
- pandas, numpy - Data processing
- tensorflow, keras - ML model
- scikit-learn - ML utilities  
- matplotlib, seaborn - Visualization
- joblib - Model persistence
- requests - API calls

## ğŸ”„ Workflow

1. **Data Collection** â†’ `main_pipeline.py`
2. **Data Cleaning** â†’ `cleaning_pipeline.py`
3. **Model Training** â†’ (done in Google Colab)
4. **Predictions** â†’ `local_inference.py`
5. **Testing** â†’ `test_project.py`

````
